{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cloud-Quiz-2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+/cadzGx9WH0ZF8PH2A3P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityaaay/adityaaay/blob/main/Cloud_Quiz_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLOUD COMPUTING QUIZ 2"
      ],
      "metadata": {
        "id": "V_9rVxQaDO5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and importing libraries"
      ],
      "metadata": {
        "id": "Uid7rHFlEMBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pyspark\n",
        "import pyspark\n",
        "from pyspark.sql.functions import when,count,col,isnan\n",
        "import pyspark.sql.functions as fun\n",
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGbGCzYKDk7q",
        "outputId": "d15c6845-91d8-4978-be8d-a14f783c7ec6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 28 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 40.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=a71442ffc329a0f6264a3e416acc74a20c1065b5f4d6e316aa0b3449e380d4fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a PySpark session for the User"
      ],
      "metadata": {
        "id": "nqDkaPYRFvJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "py_spark = pyspark.sql.SparkSession.builder\\\n",
        "        .master('local[*]')\\\n",
        "        .appName('as15136_quiz2')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "nxMOdXsxFxbJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Part\n",
        "1. Download the data.txt from [here](https://github.com/adityaaay/miscellaneous/blob/main/data.txt)\n",
        "2. The following code classifies the data into separate columns."
      ],
      "metadata": {
        "id": "qnDtlR9OECd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = py_spark.read.option(\"delimiter\", \" \").option(\"header\", \"false\").csv('data.txt')\n",
        "spark_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n0eXiLdGRLH",
        "outputId": "bf04543f-4718-41c4-db26-7ed2c29f4753"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_c0='15136', _c1='Sharma', _c2='Aditya', _c3='Tandon', _c4='IN', _c5='3.6'),\n",
              " Row(_c0='15137', _c1='Doe', _c2='John', _c3='Stern', _c4='US', _c5='4'),\n",
              " Row(_c0='15138', _c1='Walker', _c2='Paul', _c3='Steinhardt', _c4='US', _c5='4'),\n",
              " Row(_c0='15139', _c1='Disel', _c2='Vin', _c3='CUSP', _c4='FA', _c5='2.6'),\n",
              " Row(_c0='15140', _c1='Rose', _c2='Lana', _c3='Courant', _c4='FA', _c5='3.2')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark_df.withColumnRenamed(\"_c0\",\"Student_ID\")\n",
        "spark_df = spark_df.withColumnRenamed(\"_c1\",\"Last_Name\")\n",
        "spark_df = spark_df.withColumnRenamed(\"_c2\",\"First_Name\")\n",
        "spark_df = spark_df.withColumnRenamed(\"_c3\",\"Department_Name\")\n",
        "spark_df = spark_df.withColumnRenamed(\"_c4\",\"Origin_Country\")\n",
        "spark_df = spark_df.withColumnRenamed(\"_c5\",\"GPA\")\n",
        "spark_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5M7gFf9Eqen",
        "outputId": "888466e1-f672-428d-d0bc-0608cec69736"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Student_ID='15136', Last_Name='Sharma', First_Name='Aditya', Department_Name='Tandon', Origin_Country='IN', GPA='3.6'),\n",
              " Row(Student_ID='15137', Last_Name='Doe', First_Name='John', Department_Name='Stern', Origin_Country='US', GPA='4'),\n",
              " Row(Student_ID='15138', Last_Name='Walker', First_Name='Paul', Department_Name='Steinhardt', Origin_Country='US', GPA='4'),\n",
              " Row(Student_ID='15139', Last_Name='Disel', First_Name='Vin', Department_Name='CUSP', Origin_Country='FA', GPA='2.6'),\n",
              " Row(Student_ID='15140', Last_Name='Rose', First_Name='Lana', Department_Name='Courant', Origin_Country='FA', GPA='3.2')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1 : Where GPA > 3.75"
      ],
      "metadata": {
        "id": "ey_ECxREEwSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the data of GPA > 3.75 and write to file\n",
        "op1_spark = spark_df.filter(spark_df.GPA > 3.75)\n",
        "op1_spark.head(5)\n",
        "op1_spark.write.csv(\"gpa_only.csv\")"
      ],
      "metadata": {
        "id": "WXMrcGTsE3cf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2 : Where GPA > 3.75 and Categorised by Country"
      ],
      "metadata": {
        "id": "05OR5cP6E-jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "op1_spark = op1_spark.select(op1_spark.Origin_Country)\n",
        "op1_spark = op1_spark.withColumn('new_column', fun.lit(0))\n",
        "op1_spark = op1_spark.select(op1_spark.Origin_Country,op1_spark.new_column.alias(\"Count\"))\n",
        "op1_spark = op1_spark.withColumn('Count', fun.count('*').over(Window.partitionBy(op1_spark.Origin_Country, op1_spark.Count))).distinct()\n",
        "op1_spark.head(5)\n",
        "op1_spark.write.csv(\"gpa_and_country.csv\")"
      ],
      "metadata": {
        "id": "63gpUi49FGIz"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}